import cv2
import numpy as np
from skimage.util.shape import view_as_windows
import os
import fnmatch
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import scipy.cluster.vq as vq
from sklearn import svm
from collections import Counter

list_of_classes = []
hash_map = {}
index = 0
N = 0

for _, dirs, files in os.walk("../Data/training/", topdown=False):  # getting each class from data
    for name in dirs:
        list_of_classes.append(name)



def extract_patches(list_of_classes, step, patch_size) -> (np.ndarray, np.ndarray):
    '''

    Function which iterates over a set of images and turns them into a set of observations for K-means algorithm.
    After reading an image, it decomposes the image into an array of N patches with given patch size of (X by Y),
    it standardises these patches, and appends the patches into a set of observations X.

    :param list_of_classes: List of all classes of images.
    :param step: Pixel-wise distance between each patch's origin.
    :param patch_size: X by Y size of window (or patch) extracted from each image.
    :return: Data Matrix prepared for K-means.
    '''

    X, Y = [], []
    scaler = StandardScaler()

    for current_class in list_of_classes:
        for index in range(100):
            image = cv2.imread('../Data/training/' + current_class + '/' + str(index) + '.jpg', cv2.IMREAD_GRAYSCALE)
            print('../Data/training/' + current_class + '/' + str(index) + '.jpg')
            floatImage = np.float32(image)

            patches = view_as_windows(floatImage, patch_size, step=step)
            standardised_patches = []
            print(current_class, floatImage.shape)

            for x in range(patches.shape[0]):  # Standardise all patches
                for y in range(patches.shape[1]):
                    patch = scaler.fit_transform(patches[x][y])
                    # Flatten them to 1D list from 2D list.
                    standardised_patches.append(patch.flatten())
            X.append(standardised_patches)
            Y.append(current_class)

    return np.array(X), np.array(Y)


def get_histograms(features, k) -> np.ndarray:
    '''
    Receives feature matrix 'features' generated by function 'prepare for k_means', and applies K means clustering
    algorithm on each feature (image) with given K. The result of the clustering is then quantised, and turned

    :param features: Feature matrix of patches of all images.
    :param k: Number of centroids in each image.
    :return: List of all histograms (Visual Words) for each feature (image) from list of features.
    '''
    histograms = []

    for i in range(features.shape[0]):
        print(i)

        # Cluster patches of an image by K Means with given K.
        # Return the K centroids (or codebook).
        codebook, _ = vq.kmeans(obs=features[i], k_or_guess=k)
        # Vector quantisation. Map patches of an image to centroids (codebook)..
        code, _ = vq.vq(features[i], codebook)
        # Generate the bag of visual words, or histogram of visual words from the quantised code.
        hist, _ = np.histogram(code, bins=range(codebook.shape[0]+1), normed=False)
        # print(hist)
        histograms.append(hist)

    return np.array(histograms)


#Uncomment this code if you want to generate feature_matrix with different parameters.
# X, Y_train = extract_patches(list_of_classes, step=4, patch_size=(8, 8))
#print('The Shape of Descriptors for All Images is', X.shape)
#np.save('feature_matrix_1500imgs_4step_88patch.npy',feature_matrix)
#np.save('Y_labels_1500imgs.npy',Y_train)


#print('Loading feature matrix of patches from all images')
#feature_matrix = np.load('feature_matrix_1500imgs_4step_88patch.npy',allow_pickle=True)
#print('Feature matrix loaded')

# print(len(feature_matrix),'Number of Images in feature_matrix list')
# # print(len(feature_matrix[0]), 'Number of patches in first image.')
# # print('# all patches from 0th image.',feature_matrix[0])

#print('Generating Histograms for all Images')
#histograms = get_histograms(feature_matrix, k=500)
#print('Histograms done.')
#np.save('histograms_k500_1500imgs_4_step_88patch.npy',histograms)
#exit()

print('Loading generated histograms. And their labels')
histograms = np.load('histograms_k500_1500imgs_4_step_88patch.npy', allow_pickle=True)
Y_labels = np.load('Y_labels_1500imgs.npy')
print('Histograms and labels loaded')

print('Print first histogram and plot it')
print(histograms[0])
print(Counter(histograms[0]))
plt.hist(histograms[0],bins=500)
plt.show()


## Train models
#Classification Stage

# Prepare training and test data
X_train, X_test, y_train, y_test = train_test_split(histograms, Y_labels, test_size=0.3, random_state=42)